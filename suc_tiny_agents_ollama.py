# -*- coding: utf-8 -*-
"""suc_tiny-agents_ollama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUXZTWUzLgMc3Ytgp8SYVp8OLyjWqtb0
"""



"""https://huggingface.co/docs/hub/en/agents#tiny-agents-js-and-python"""

!pip install "huggingface_hub[mcp]>=0.32.2"



"""شغال"""

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
        {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

"""شغال"""

!tiny-agents run ./my-agent



"""شغال%%%%%%%%%%%%%%%"""

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
  ]
}

!tiny-agents run ./my-agent

"""%%%%%%%%%%%%%%%%%%%%%"""





























! tiny-agents run --help

!git clone https://github.com/huggingface/huggingface_hub.git

# Commented out IPython magic to ensure Python compatibility.
# %cd huggingface_hub

!git fetch origin pull/3242/head:tiny-agents-pr
!git checkout tiny-agents-pr

!ls agent/

git clone https://github.com/huggingface/huggingface_hub.git
cd huggingface_hub
git fetch origin pull/3242/head:tiny-agents-pr
git checkout tiny-agents-pr

!find . -name "*agent*"

# File: main.py
from tinyagent import TinyAgent
from ollama_client import OllamaClient

# 1. إنشاء عميل Ollama يشير إلى النموذج المحلي
llm = OllamaClient(model="llama2")  # غيّر "llama2" إلى اسم النموذج الذي حملته

# 2. تهيئة الوكيل (Agent) مع عميل LLM المخصص
agent = TinyAgent(llm=llm)

# 3. إعداد المحادثة (System + User)
messages = [
    {"role": "system",  "content": "أنت مساعد ذكي يتحدث العربية."},
    {"role": "user",    "content": "مرحبا، كيف حالك؟"},
]

# 4. إرسال المحادثة واستلام الرد
response = agent.chat(messages)

# 5. طباعة الرد
print(response)

# File: ollama_client.py
import subprocess
from typing import List, Dict

class OllamaClient:
    """
    Simple local LLM client using Ollama CLI.
    Expects 'ollama' to be installed and a model pulled (e.g., llama2).
    """
    def __init__(self, model: str):
        self.model = model

    def chat(self, messages: List[Dict]) -> List[Dict]:
        """
        messages: list of {"role": str, "content": str}
        Returns assistant messages in same format.
        """
        # Build a single prompt from chat messages
        prompt = ""
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            # Ollama eval uses plain prompt, you can format roles as needed
            if role == "system":
                prompt += f"[INST] <<SYS>>\n{content}\n<</SYS>>\n"
            else:
                # For user/assistant, wrap in INST tags
                if role == "user":
                    prompt += f"[INST] {content} [/INST]\n"
                else:
                    # assistant messages are usually not re-sent in prompt
                    pass

        # Call ollama CLI
        try:
            result = subprocess.run(
                ["ollama", "eval", self.model, "--stdin"],
                input=prompt,
                text=True,
                capture_output=True,
                check=True
            )
            response_text = result.stdout.strip()
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Ollama CLI failed: {e.stderr}")

        # Return as assistant message
        return [{"role": "assistant", "content": response_text}]

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!pip install tinyagent

!nohup ollama serve &
!ollama pull llama2

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!pip install git+https://github.com/askbudi/tinyagent.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!nohup ollama serve &
!python main.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/huggingface_hub/src/huggingface_hub

!python /content/huggingface_hub/src/huggingface_hub/inference/_mcp/agent.py

from huggingface_hub.inference._mcp.agent import Agent  # إذا كان هناك كلاس بهذا الاسم

!pip install huggingface_hub

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/huggingface_hub
!python -m huggingface_hub.inference._mcp.agent

!python -m huggingface_hub.inference._mcp.agent

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/huggingface_hub
!python -m huggingface_hub.inference._mcp.agent

from huggingface_hub.inference._mcp.agent import Agent  # إذا كان اسم الكلاس Agent
agent = Agent(...)
result = await agent.run(...)

from tinyagent import TinyAgent

from tinyagent.tiny_agent import TinyAgent

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!nohup ollama serve &
!python main.py

!ollama list

!pip install modal

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!nohup ollama serve &
!python main.py

!pip install tinyagent-py[all]

import asyncio
import os
from tinyagent import TinyAgent
from tinyagent.tools.subagent import create_general_subagent
from tinyagent.tools.todo_write import enable_todo_write_tool

from tinyagent.code_agent.example import run_example
import asyncio

# Run the full example with Gradio interface
asyncio.run(run_example())

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!nohup ollama serve &
!python main.py

import asyncio
from tinyagent import TinyCodeAgent
from textwrap import dedent

# تهيئة TinyCodeAgent لاستخدام نموذج Ollama المحلي
# "ollama/llama3" يخبر LiteLLM باستخدام نموذج llama3 من خادم Ollama
agent = TinyCodeAgent(model="ollama/llama2")

async def main():
    # مثال على مهمة يمكن للـ agent تنفيذها
    # سيقوم الـ agent بكتابة وتنفيذ كود بايثون لحل هذه المهمة
    response = await agent.run(dedent("""
    اقترح لي 13 وسمًا (tags) لقائمة منتجاتي على Etsy،
    يجب أن يكون كل وسم متعدد الكلمات وبحد أقصى 20 حرفًا.
    يجب استخدام كل كلمة مرة واحدة فقط في جميع الوسوم.
    يجب أن تغطي الوسوم طرقًا مختلفة يبحث بها الأشخاص عن المنتج على Etsy.
    - يجب عليك استخدام قدراتك في البرمجة للتحقق من أن إجابتك تفي بالمعايير
    ومواصلة عملك حتى تصل إلى الإجابة.

    منتجي هو **مجموعة دعوات زفاف مكونة من 3 قطع، باللون الأخضر المريمي، مع إطار من ورق الذهب.**
    """), max_turns=20)

    print(response)

# تشغيل الدالة الرئيسية بشكل غير متزامن
await main()

"""https://huggingface.co/docs/hub/en/agents#tiny-agents-js-and-python"""

import asyncio
from tinyagent import TinyCodeAgent
from textwrap import dedent


# Initialize TinyCodeAgent to use a local Ollama model
# "ollama/llama3" tells LiteLLM to use the llama3 model from the Ollama server
agent = TinyCodeAgent(model="ollama/llama2")

async def main():
    # Example of a task the agent can perform
    # The agent will write and execute Python code to solve this task
    response = await agent.run(dedent("""
    Suggest 13 tags for my Etsy product listing.
    Each tag must be multi-word and a maximum of 20 characters.
    Each word should only be used once across all tags.
    The tags should cover different ways people search for the product on Etsy.
    - You must use your programming capabilities to verify that your answer meets the criteria
    and continue your work until you arrive at the answer.

    My product is a **3-piece wedding invitation suite, in sage green, with a gold foil frame.**
    """), max_turns=3)

    print(response)

# Run the main function asynchronously
await main()

/content/my-agent/agent.json

{
"model": "llama2:latest",
"endpointUrl": "http://localhost:11434/",
"servers": [
{
"type": "sse",
"config": {
"url": "http://127.0.0.1:7860/gradio_api/mcp/sse"
}
}
]
}

!ollama list

!tiny-agents run /content/my-agent/agent.json

https://github.com/sdelahaies/tiny-agent-mcp/blob/main/my-agent/agent.json

!nohup ollama serve &

!ollama list

!git clone https://github.com/sdelahaies/tiny-agent-mcp.git

# Commented out IPython magic to ensure Python compatibility.
# %cd tiny-agent-mcp

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent --url "https:evalstate-hf-mcp-server.hf.space/mcp"

!tiny-agents run ./my-agent --url "https://evalstate-hf-mcp-server.hf.space/mcp"

!tiny-agents run --help

!tiny-agents run /content/tiny-agent-mcp/my-agent/agent.json

!!pip install huggingface-hub mcp-client tiny-agents

from huggingface_hub import notebook_login
notebook_login()  # يُفتح نافذة لتسجيل الدخول

!pip install "huggingface_hub[mcp]>=0.32.0"

from tiny_agents import Agent

# تعريف وكيل بسيط
agent = Agent(
    model="gpt2",  # أو أي نموذج آخر مثل "llama2"
    tools=["search", "python-code-execution"]  # الأدوات المتاحة
)

# تشغيل الوكيل
response = agent.run("ما هو الطقس اليوم في باريس؟")
print(response)



!pip install tinyagent

!mcp-server start --port 8000

!tiny-agents run ./my-agent

!ollama list

"""{
  "model": "llama2:latest",
  "endpointUrl": "http://localhost:11434",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

"""

!tiny-agents run ./my-agent

{
  "model": "llama2:latest",
  "endpointUrl": "http://localhost:11434",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!tiny-agents run ./my-agent

!curl http://localhost:11434/api/generate -d '{
"model": "llama2:latest",
"prompt": "Hello!"
}'



!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

"""ابحث لي عن نموذج Stable Diffusion
bash
Copy
Edit
اعرض تفاصيل نموذج stabilityai/stable-diffusion-3
Copy
Edit
ابحث عن مجموعة بيانات للترجمة من الإنجليزية إلى العربية
Copy
Edit
ولد لي صورة لقطة ترتدي نظارة شمسية
Copy
Edit
اعرض من أنا في Hugging Face
إذا أردت أقدر أكتب لك قائمة أوامر جاهزة لكل أداة بحيث تجربها مباشرة.
"""

!nohup ollama serve &

!ollama list

!ollama run llama2:latest

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!curl http://localhost:11434/api/chat -d '{

"model": "llama2:latest",
"messages": [
    {
      "role": "user",
      "content": "Why is the sky blue?"
    }
  ]
}'

!curl http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
  "model": "llama2:latest",
  "messages": [
    {
      "role": "user",
      "content": "Why is the sky blue?"
    }
  ]
}'

!!curl http://localhost:11434/api/chat -H "Content-Type:  application/json" -d '{
  "model": "llama2:latest",
  "messages": [
    {
      "role": "user",
      "content": "Why is the sky blue?"
    }
  ]
}'

!tiny-agents run ./my-agent

!nohup ollama serve &

!pkill ollama

import subprocess
import time

# Start Ollama server in the background
process = subprocess.Popen(["ollama", "serve"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print("Starting Ollama server...")
# Give it a few seconds to initialize
time.sleep(5)
print("Ollama server should be running.")

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!nohup ollama serve &
!tiny-agents run ./my-agent

!nohup ollama serve &
!tiny-agents run ./my-agent







# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/alamriku/summarize-pdf.git

# Commented out IPython magic to ensure Python compatibility.
# %cd summarize-pdf

// Connect Ollama locally
  const llm = new ChatOllama({
    baseUrl: "http://localhost:11434",
    model: "llama2:latest", // or mistral, gemma, etc.
  });

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/my-agent

!nohup ollama serve &
!tiny-agents run ./my-agent

!nohup ollama serve &
!ollama pull llama3.2:3b

"""https://github.com/huggingface/huggingface_hub/releases

https://github.com/huggingface/huggingface.js/issues/1502
"""

https://github.com/huggingface/huggingface.js/issues/1502

!ollama list

"""شغال%%%%%%%%%%%%%"""

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
  ]
}

!tiny-agents run ./my-agent

"""%%%%%%%%%%%%%%%%%%%%%"""





{
  "model": "openai/llama3.2:3b",
  "endpointUrl": "http://localhost:11434/v1",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!pip install huggingface_hub>=0.32.2

!pip show huggingface_hub

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!tiny-agents run ./my-agent

{
"model": "llama3.2:3b",
"endpointUrl": "http://localhost:11434/",
"provider": "auto",
"servers": [
{
"type": "sse",
"config": {
"url": "http://127.0.0.1:7860/gradio_api/mcp/sse"
}
}
]
}

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434/",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space"
    }
  ]
}

!tiny-agents run ./my-agent

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434/",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent

!tiny-agents run ./my-agent



!nohup ollama serve &
!ollama list

import asyncio
from tinyagent import TinyCodeAgent
from tinyagent.llm_client import LLMClient # Import the base LLMClient class
from textwrap import dedent

# Re-define the OllamaClient class
class OllamaClient(LLMClient):
    async def chat_completion(self, messages, stream=False, **kwargs):
        # Simplified interaction - you might need to adjust this based on Ollama's API
        # For chat, Ollama's API is at /api/chat
        url = f"{self.base_url}/api/chat"
        headers = {"Content-Type": "application/json"}

        # Convert messages to Ollama's chat format
        ollama_messages = []
        for msg in messages:
            ollama_messages.append({"role": msg['role'], "content": msg['content']})

        payload = {
            "model": self.model_name,
            "messages": ollama_messages,
            "stream": stream, # Respect the stream parameter
            **kwargs # Include any other parameters passed to the method
        }

        async with self._client.post(url, json=payload, headers=headers) as response:
             # Raise an exception for bad status codes (4xx or 5xx)
            response.raise_for_status()

            if stream:
                async for line in response.content.iter_any():
                     # Assuming Ollama streams JSON objects line by line
                    if line:
                        try:
                            data = json.loads(line)
                            # Yield chunks that look like OpenAI streaming response chunks
                            if 'message' in data and 'content' in data['message']:
                                yield {"choices": [{"delta": {"content": data['message']['content']}}]}
                            elif 'done' in data:
                                yield {"choices": [{"delta": {}}], "done": True}
                        except json.JSONDecodeError:
                            print(f"Could not decode JSON from stream: {line}")
                            continue

            else:
                data = await response.json()
                # Format the response to look like a non-streaming OpenAI response
                if 'message' in data and 'content' in data['message']:
                    return {"choices": [{"message": {"role": "assistant", "content": data['message']['content']}}]}
                else:
                    # Handle unexpected response format
                    print(f"Unexpected non-streaming response format: {data}")
                    return {"choices": [{"message": {"role": "assistant", "content": "Error: Unexpected response format"}}]}


# Initialize TinyCodeAgent with the custom OllamaClient
# Make sure the base_url points to your Ollama server
# and the model_name matches the model you pulled
ollama_base_url = "http://localhost:11434" # Update if your Ollama server is on a different address/port
ollama_model_name = "llama2:latest" # Update if you want to use a different model like llama3.2:3b

ollama_client_instance = OllamaClient(base_url=ollama_base_url, model_name=ollama_model_name)

# Pass the instance of the custom client to TinyCodeAgent
agent = TinyCodeAgent(llm_client=ollama_client_instance)


async def main():
    # Example task for the agent
    user_task = dedent("""
    اقترح لي 13 وسمًا (tags) لقائمة منتجاتي على Etsy،
    يجب أن يكون كل وسم متعدد الكلمات وبحد أقصى 20 حرفًا.
    يجب استخدام كل كلمة مرة واحدة فقط في جميع الوسوم.
    يجب أن تغطي الوسوم طرقًا مختلفة يبحث بها الأشخاص عن المنتج على Etsy.
    - يجب عليك استخدام قدراتك في البرمجة للتحقق من أن إجابتك تفي بالمعايير
    ومواصلة عملك حتى تصل إلى الإجابة.

    منتجي هو **مجموعة دعوات زفاف مكونة من 3 قطع، باللون الأخضر المريمي، مع إطار من ورق الذهب.**
    """)

    print("Running agent...")
    response = await agent.run(user_task, max_turns=10)
    print("\nAgent finished. Response:")
    print(response)

# Run the main function using await at the top level
await main()

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434/",
  "servers": [
    {
      "type": "http",
      "url": "http://localhost:11434/mcp"
    }
  ]
}

!tiny-agents run ./my-agent

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!tiny-agents run ./my-agent

"""ThinkingThoughts
(experimental)
Auto
Expand to view model thoughts

chevron_right
ThinkingThoughts
(experimental)
Auto
Expand to view model thoughts

chevron_right

"""

import os
import subprocess
import time

# أولاً، أوقف أي عملية قديمة لضمان بداية نظيفة
print("Stopping any old Ollama processes...")
os.system("pkill -f ollama")
time.sleep(2)

# ثانيًا، أعد تشغيل الخادم في وضع التوافق مع OpenAI
print("Restarting Ollama server...")
process = subprocess.Popen(["ollama", "serve"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
time.sleep(10) # امنحه 10 ثوانٍ للبدء بشكل كامل
print("✅ Server restarted! You can now run your agent again.")

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434/v1",
  "servers": [
    {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}




{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434/v1",
  "servers": [
    {
      "type": "sse",
      "url": "http://127.0.0.1:7860/gradio_api/mcp/sse"
    }
  ]
}



http://127.0.0.1:7860/gradio_api/mcp/sse

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
        {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}















"""### شغال"""

ollama serve
ف الترمنال

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
        {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

{
  "model": "llama3.2:3b",
  "endpointUrl": "http://localhost:11434",
  "servers": [
        {
      "type": "http",
      "url": "https://evalstate-hf-mcp-server.hf.space/mcp"
    }
  ]
}

!tiny-agents run ./my-agent